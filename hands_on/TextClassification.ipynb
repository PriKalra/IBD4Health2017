{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the materials including this notebook are available in Github: [parantapag/IBD4Health2017](https://github.com/parantapag/IBD4Health2017)\n",
    "\n",
    "## Vector Space Model\n",
    "We are interested in using this data to build statistical models. So, we now need to **vectorize** this data. The goal is to find a way to represent the data so that the computer can understand it.\n",
    "\n",
    "### Bag of words\n",
    "A bag of words represents each document in a corpus as a series of features. Most commonly, the features are the collection of all unique words in the vocabulary of the entire corpus. The values are usually the count of the number of times that word appears in the document (term frequency). A corpus is then represented as a matrix with one row per document and one column per unique word.\n",
    "\n",
    "### Scikit-Learn\n",
    "[Scikit-learn](http://scikit-learn.org/stable/) is machine learning library for the Python programming language. It features a wide range of machine learning algorithms for classification, regression and clustering. It also provides various supporting machine learning techniques such as cross validation, text vectorizer. Scikit-learn is designed to interoperate with the Python numerical and scientific libraries [NumPy](http://www.numpy.org/).\n",
    "\n",
    "Simple to use: import the required module and call it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer\n",
    "To build our initial bag of words count matrix, we will use scikit-learn's CountVectorizer class to transform our corpus into a bag of words representation. CountVectorizer expects as input a list of raw strings containing the documents in the corpus. It takes care of the tokenization, transformation to lowercase, filtering stop words, building the vocabulary etc. It also tabulates occurrance counts per document for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "raw_docs_sample = [\"The dog sat on the mat.\", \"The cat sat on the mat!\", \"We have a mat in our house.\"]\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words='english')  \n",
    "X_sample = vectorizer.fit_transform(raw_docs_sample)\n",
    "X_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse Vs Dense Matrices\n",
    "Dense matrices store every entry in the matrix. Sparse matrices only store the nonzero entries. Sparse matrices don't have a lot of extra features, and some algorithms may not work for them. You use them when you need to work with matrices that would be too big for the computer to handle them, but they are mostly zero, so they compress easily. Be aware of issues that may arise at:\n",
    "- dot product\n",
    "- slicing (row, column)\n",
    "\n",
    "In python these are taken care almost automatically, by using sparse dot product and implementations of csr and csc matrices (`scipy.sparse.csr_matrix`, `scipy.sparse.csc_matrix`, etc..). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Count Matrix:\")\n",
    "print(X_sample.todense())\n",
    "print(\"\\nWords in vocabulary:\")\n",
    "print(vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Weighting Scheme\n",
    "The tf-idf weighting scheme is frequently used text mining applications and has been shown to be effective. It combines local (term frequency or tf) and global(inverse document frequency of idf) term statistics. \n",
    "\n",
    "Scikit-learn has your back, it already provides the module to compute TF-IDF matrix.\n",
    "\n",
    "Note: Scikit-learn uses a slightly different formula than that we saw today morning. You can refer to [corresponding documentation](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) to know more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "tfidf = TfidfTransformer()  \n",
    "X_tfidf_sample = tfidf.fit_transform(X_sample)\n",
    "print(\"TF-IDF Matrix:\\n\")\n",
    "print(X_tfidf_sample.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Bigger Collection\n",
    "We will use [**Ohsumed** collection](ftp://medir.ohsu.edu/pub/ohsumed). It includes medical abstracts from the MeSH categories of the year 1991. Each abstract is assigned one of [*23 categories*](http://disi.unitn.it/moschitti/corpora/First-Level-Categories-of-Cardiovascular-Disease.txt). Here is an example of an abstract under *Bacterial Infections and Mycoses* catgory:\n",
    ">Replacement of an aortic valve cusp after neonatal endocarditis.\n",
    ">Septic arthritis developed in a neonate after an infection of her hand.\n",
    ">Despite medical and surgical treatment endocarditis of her aortic valve developed and the resultant regurgitation required emergency surgery.\n",
    ">At operation a new valve cusp was fashioned from preserved calf pericardium.\n",
    ">Nine years later she was well and had full exercise tolerance with minimal aortic regurgitation.\n",
    "\n",
    "In this hands-on we will use 1641 documents belonging to two categories, namely *Bacterial Infections and Mycoses* and *Musculoskeletal Diseases*.\n",
    "\n",
    "The file **corpus.txt** supplied here, contains 1641 documents. Each line of the file is a document.\n",
    "\n",
    "Now we will:\n",
    "   1. Load the documents as a list\n",
    "   2. Create TF-IDF vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw_docs = open(\"corpus.txt\").read().splitlines()\n",
    "print(\"Loaded \" + str(len(raw_docs)) + \" documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write code to vectorize the raw documents to count matrix\n",
    "\n",
    "\n",
    "# Write code to create TF-IDF vectors\n",
    "# Store the TF-IDF matrix in a variable named X_tfidf\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classifier\n",
    "Machine learning algorithms need a training set. In our text classification scenario, we need category or class labels for all 1641 documents in the collection.\n",
    "\n",
    "In our collection we have documents from two categories: \"Bacterial Infections and Mycoses\" (category C01) and  \"Musculoskeletal Diseases\" (category C05). For each document we know the labels. The labels are stored in **corpus_labels.txt** file. Each line of **corpus.txt** file corresponds to the label in the same line of file **corpus_labels.txt**.\n",
    "\n",
    "Lets load the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = open(\"corpus_labels.txt\").read().splitlines()\n",
    "print(\"Loaded \" + str(len(labels)) + \" labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of simplicity, we will assume numerical labels for the categories. So labels 'C01' is replaces with numeric 1.0 and labels 'C05' are replaced with numeric -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace string labels with numerical ones\n",
    "y = np.array([1.0 if label==\"C01\" else -1.0 for label in labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing\n",
    "\n",
    "As we wish to first train a model and then to see how well it is. So the norm is to divide the data into two parts:\n",
    " 1. **Training set:** Documents along with their class labels are used to train the model.\n",
    " 2. **Test set:** Documents are used for predicting the class labels using the trained classifier. However, the class labels of this set are kept *hidden* and are only revealed during evalutaion of the trained model, not before that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# package to split training and testing data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split the data into training and testing\n",
    "X_train, X_test, y_train, y_test =train_test_split(X_tfidf, y, test_size = 0.2, random_state = 256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# train the classifier\n",
    "classifierNB = MultinomialNB()  \n",
    "classifierNB.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "Now we will test the trained classifier on out kept hidden test data to see how well it did. \n",
    "\n",
    "Here we will look at the accuracy of the model, the simplest evaluation measure used in machine learning algorithms: $$accuracy = \\frac{\\text{number of correctly classified examples}}{\\text{total number of examples}}$$\n",
    "\n",
    "There are more informative and complex evaluaton measures, e.g. precision, recall, f-measure etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Package for reporting accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# predict labels for the test set\n",
    "predictionsNB = classifierNB.predict(X_test.toarray())\n",
    "\n",
    "# report accuracy\n",
    "accuracyNB = accuracy_score(y_test, predictionsNB)\n",
    "print(\"Test accuracy: \" + str(accuracyNB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Classifiers\n",
    "You have virtually an endless option to choose your classifier. Let's try some more.\n",
    "\n",
    "Method is simple: import and call the package\n",
    "\n",
    "#### Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import package\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Write code to create and train a SVM classifier\n",
    "classifierSVM = ...\n",
    "\n",
    "\n",
    "# Write code to predict labels for the test set\n",
    "predictionsSVM = ...\n",
    "\n",
    "# Write code to report accuracy\n",
    "accuracySVM = ...\n",
    "print(\"Test accuracy: \" + str(accuracySVM))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import package\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Write code to create and train a Linear Regression classifier\n",
    "classifierRF = ...\n",
    "\n",
    "\n",
    "# Write code to predict labels for the test set\n",
    "predictionsRF = ...\n",
    "\n",
    "# Write code to report accuracy\n",
    "accuracyRF = ...\n",
    "print(\"Test accuracy: \" + str(accuracyRF))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
